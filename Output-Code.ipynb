{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\SJ\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SJ/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\SJ/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\SJ/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tools...\n",
      "WARNING:tensorflow:From C:\\Users\\SJ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\SJ\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and scaler loaded successfully.\n",
      "Input: 'the washington post tried to compare sen. elizabet...' -> Predicted: Fake\n",
      "Input: 'the washington post tried to compare sen. elizabet...' -> Predicted: Fake\n",
      "Input: 'in preparation for the scheduled 2018 launch of th...' -> Predicted: Fake\n",
      "Input: 'posted on october 30, 2016 by sean adl-tabatabai i...' -> Predicted: Real\n",
      "Input: '...' -> Predicted: Real\n",
      "Input: 'donald trump...' -> Predicted: Real\n",
      "Fetched 10 articles for 'donald trump'.\n",
      "Text: 'The United States of Elon Musk Inc. Where do Elon ...' -> Predicted: Fake\n",
      "Text: 'Donald Trump wants to delete ‘climate’ from federa...' -> Predicted: Fake\n",
      "Text: 'Fired Democratic FTC commissioners sue Trump ﻿Kell...' -> Predicted: Fake\n",
      "Text: 'Donald Trump Bought a $90,000 Tesla With 37 Recall...' -> Predicted: Fake\n",
      "Text: 'The US Solar Power Industry Is Trying to Rebrand a...' -> Predicted: Fake\n",
      "Text: 'People Are Paying Millions to Dine With Donald Tru...' -> Predicted: Fake\n",
      "Text: 'Donald Trump Held Another Million-Dollar 'Candleli...' -> Predicted: Fake\n",
      "Text: 'Netanyahu Gives John Fetterman Silver-Plated Pager...' -> Predicted: Real\n",
      "Text: 'Trump pardons ex-Nikola CEO Trevor Milton Milton w...' -> Predicted: Real\n",
      "Text: 'Senate Votes to Strip CFPB of Ability to Regulate ...' -> Predicted: Fake\n",
      "Input: 'donald j. trump overhauled his transition team fro...' -> Predicted: Fake\n",
      "Input: 'donald j. trump overhauled his transition team fro...' -> Predicted: Fake\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "print(\"Installing packages...\")\n",
    "!pip install newsapi-python -q\n",
    "print(\"Packages installed.\")\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import zipfile\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "# print(\"Checking NLTK wordnet resource...\")\n",
    "# wordnet_zip = '/kaggle/working/nltk_data/corpora/wordnet.zip'\n",
    "# if os.path.exists(wordnet_zip):\n",
    "#     print(f\"Unzipping wordnet from {wordnet_zip}...\")\n",
    "#     with zipfile.ZipFile(wordnet_zip, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('/kaggle/working/nltk_data/corpora')\n",
    "#     os.remove(wordnet_zip)\n",
    "#     print(\"Wordnet unzipped and zip file removed.\")\n",
    "# else:\n",
    "#     print(\"Wordnet already unzipped or not downloaded as zip.\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "import pickle\n",
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# Define a minimal Cast layer as a fallback (only if needed)\n",
    "class Cast(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Cast, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Let TensorFlow infer dtype from inputs or model context\n",
    "        return tf.cast(inputs, dtype=self.dtype_policy.compute_dtype)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(Cast, self).get_config()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# Robust file checking and loading\n",
    "def load_file(file_path, load_func, desc):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            return load_func(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {desc} from {file_path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"{desc} file not found at {file_path}.\")\n",
    "        return None\n",
    "\n",
    "# Load model and tools with error handling\n",
    "print(\"Loading model and tools...\")\n",
    "model_path = 'output/lstm_model.h5'\n",
    "tokenizer_path = 'output/tokenizer.pkl'\n",
    "scaler_path = 'output/scaler.pkl'\n",
    "\n",
    "with custom_object_scope({'Cast': Cast}):\n",
    "    model = load_file(model_path, lambda p: tf.keras.models.load_model(p), \"Model\")\n",
    "tokenizer = load_file(tokenizer_path, lambda p: pickle.load(open(p, 'rb')), \"Tokenizer\")\n",
    "scaler = load_file(scaler_path, lambda p: pickle.load(open(p, 'rb')), \"Scaler\")\n",
    "\n",
    "if model is None or tokenizer is None or scaler is None:\n",
    "    print(\"Critical files missing or corrupted. Please ensure training script ran successfully.\")\n",
    "    raise SystemExit(1)\n",
    "print(\"Model, tokenizer, and scaler loaded successfully.\")\n",
    "\n",
    "# Constants\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "NUM_FEATURES = 10\n",
    "API_KEY = '20a033afa85e4b72af903562634d7f6d'  # Replace with your NewsAPI key\n",
    "stop_words = set(stopwords.words('english')) - {'not'}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "# Preprocessing and feature extraction with fallbacks\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    try:\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text.lower())\n",
    "        words = text.split()\n",
    "        return \" \".join(lemmatizer.lemmatize(word) for word in words if word not in stop_words or word == 'not')\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_numerical_features(text):\n",
    "    if not isinstance(text, str):\n",
    "        return np.zeros(NUM_FEATURES)\n",
    "    try:\n",
    "        words = text.split()\n",
    "        title = text[:50]  # Rough title approximation\n",
    "        return np.array([\n",
    "            len(title.split()), len(words), len(title), len(text),\n",
    "            sum(1 for c in title if c.isupper()) / len(title) if len(title) > 0 else 0,\n",
    "            sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0,\n",
    "            sum(1 for c in title if c in string.punctuation), sum(1 for c in text if c in string.punctuation),\n",
    "            sia.polarity_scores(title)['compound'], sia.polarity_scores(text)['compound']\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return np.zeros(NUM_FEATURES)\n",
    "\n",
    "# Batch prediction with GPU and error handling\n",
    "def predict_batch(texts):\n",
    "    try:\n",
    "        processed_texts = [preprocess_text(t) for t in texts]\n",
    "        seqs = tokenizer.texts_to_sequences(processed_texts)\n",
    "        padded_seqs = pad_sequences(seqs, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "        num_features = scaler.transform(np.array([extract_numerical_features(t) for t in texts]))\n",
    "        with tf.device('/GPU:0'):\n",
    "            preds = model.predict([padded_seqs, num_features], batch_size=256, verbose=0)\n",
    "        return (preds >= 0.5).astype(int).flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return np.zeros(len(texts), dtype=int)  # Fallback to all 'Fake' if prediction fails\n",
    "\n",
    "# Fetch news with robust error handling\n",
    "def fetch_news(topic):\n",
    "    newsapi = NewsApiClient(api_key=API_KEY)\n",
    "    try:\n",
    "        response = newsapi.get_everything(q=topic, language='en', page_size=10)\n",
    "        articles = []\n",
    "        for article in response['articles']:\n",
    "            try:\n",
    "                resp = requests.get(article['url'], timeout=5)\n",
    "                soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "                text = \" \".join(p.get_text() for p in soup.find_all('p'))\n",
    "                if text.strip():\n",
    "                    articles.append(f\"{article['title']} {text}\")\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Failed to fetch article {article.get('url', 'unknown')}: {e}\")\n",
    "                continue\n",
    "        print(f\"Fetched {len(articles)} articles for '{topic}'.\")\n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news: {e}\")\n",
    "        return []\n",
    "\n",
    "# Interactive workflow with robustness\n",
    "while True:\n",
    "    try:\n",
    "        choice = input(\"Enter 'news' for news fetch or text to classify (or 'exit'): \").strip().lower()\n",
    "        if choice == 'exit':\n",
    "            break\n",
    "        elif choice == 'news':\n",
    "            topic = input(\"Enter news topic (e.g., Chandrayaan): \").strip()\n",
    "            if topic:\n",
    "                articles = fetch_news(topic)\n",
    "                if articles:\n",
    "                    labels = predict_batch(articles)\n",
    "                    for text, label in zip(articles, labels):\n",
    "                        print(f\"Text: '{text[:50]}...' -> Predicted: {'Real' if label else 'Fake'}\")\n",
    "                else:\n",
    "                    print(\"No articles fetched. Try another topic or check API key.\")\n",
    "        else:\n",
    "            labels = predict_batch([choice])\n",
    "            print(f\"Input: '{choice[:50]}...' -> Predicted: {'Real' if labels[0] else 'Fake'}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting gracefully...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Workflow error: {e}. Continuing...\")\n",
    "\n",
    "print(\"Program terminated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
